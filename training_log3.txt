nohup: ignoring input
================================================================================
CASCADE FAILURE PREDICTION - TRAINING SCRIPT (IMPROVED)
================================================================================

Configuration:
  Data directory: data
  Output directory: checkpoints
  Batch size: 8
  Epochs: 100
  Learning rate: 0.0001
  Device: cuda
  Gradient clipping: 20.0
  Mixed precision: True
  Resume training: True

Loading datasets...
Indexing scenarios from: data/train
Loading labels from cache: data/train/metadata_cache.json
Physics normalization: base_mva=100.0, base_frequency=60.0
Indexed 2700 scenarios.
  Cascade scenarios: 905 (33.5%)
  Normal scenarios: 1795 (66.5%)
Ultra-memory-efficient mode: Loading 1 file per sample.
Indexing scenarios from: data/val
Loading labels from cache: data/val/metadata_cache.json
Physics normalization: base_mva=100.0, base_frequency=60.0
Indexed 491 scenarios.
  Cascade scenarios: 170 (34.6%)
  Normal scenarios: 321 (65.4%)
Ultra-memory-efficient mode: Loading 1 file per sample.
  Training samples: 2700
  Validation samples: 491
  Mode: full_sequence (utilizing 3-layer LSTM for temporal modeling)

Computing sample weights for balanced sampling...
  Positive samples: 905 (33.5%)
  Negative samples: 1795 (66.5%)
  Calculated weights -> Pos: 2.98, Neg: 1.50 (creates ~1:1 batch balance)

Initializing model...
  Total parameters: 806,463
  Trainable parameters: 806,463

================================================================================
STARTING DYNAMIC LOSS WEIGHT CALIBRATION
================================================================================
Running loss calibration for 20 batches...
  Average raw loss components (unweighted):
    frequency      :   1.404782
    powerflow      :   2.122359
    prediction     :   0.145835
    risk           :   0.128636
    temperature    :   0.094621
    timing         :   0.050032
================================================================================
CALIBRATION COMPLETE
================================================================================

Balancing loss weights dynamically...
  Target Magnitude (from prediction loss): 0.1458

  Final Loss Weights (Fully Dynamic):
  Component       | Raw Loss     | Final Lambda | Initial Weighted Loss
  --------------- | ------------ | ------------ | --------------------
  Timing          | 0.0500       | 2.9149       |       0.1458
  Powerflow       | 2.1224       | 0.0687       |       0.1458
  Temperature     | 0.0946       | 1.5413       |       0.1458
  Reactive        | 0.0000       | 1.0000       |       0.0000
  Voltage         | 0.0000       | 1.0000       |       0.0000
  Frequency       | 1.4048       | 0.1038       |       0.1458
  Risk            | 0.1286       | 1.1337       |       0.1458

✓ PhysicsInformedLoss initialized with FINAL dynamic weights.
Loading checkpoint from checkpoints/latest_checkpoint.pth...
✓ Resumed from epoch 28
✓ Loaded thresholds: cascade=0.250, node=0.400

[PHASE 2 RESET] Manually resetting Learning Rate to 0.0001...
[PHASE 2 RESET] Scheduler reset.

================================================================================
STARTING TRAINING
================================================================================


Epoch 29/100
--------------------------------------------------------------------------------
Training:   0%|          | 0/338 [00:00<?, ?it/s]
================================================================================
MODEL OUTPUT VALIDATION (First Batch)
================================================================================
Checking required outputs for loss calculation...
  ✓ failure_probability: shape (8, 118, 1) (Matches expected)
  ✓ voltages: shape (8, 118, 1) (Matches expected)
  ✓ angles: shape (8, 118, 1) (Matches expected)
  ✓ line_flows: shape (8, 686, 1) (Matches expected)
  ✓ frequency: shape (8, 1, 1) (Matches expected)
  ✓ risk_scores: shape (8, 118, 7) (Matches expected)
  ✓ cascade_timing: shape (8, 118, 1) (Matches expected)
  ✓ temperature: shape (8, 118, 1) (Matches expected)

Checking other model outputs...
  ✓ reactive_flows: shape (8, 686, 1) (Matches expected)

Temporal sequence detected: B=8, T=52, N=118
  ✓ 3-layer LSTM IS BEING UTILIZED.
================================================================================

Training (Loss: 1.5270, Grad: 3.94):   0%|          | 0/338 [00:51<?, ?it/s]Training (Loss: 1.5270, Grad: 3.94):   0%|          | 0/338 [00:52<?, ?it/s, cF1=0.667, nF1=0.500, rMSE=0.003, pL=0.008, tL=0.000]Training (Loss: 3.6038, Grad: 7.07):   0%|          | 0/338 [00:53<?, ?it/s, cF1=0.667, nF1=0.500, rMSE=0.003, pL=0.008, tL=0.000]Training (Loss: 3.6038, Grad: 7.07):   0%|          | 0/338 [00:53<?, ?it/s, cF1=0.815, nF1=0.511, rMSE=0.005, pL=0.015, tL=0.095]Training (Loss: 1.6879, Grad: 3.71):   0%|          | 0/338 [00:53<?, ?it/s, cF1=0.815, nF1=0.511, rMSE=0.005, pL=0.015, tL=0.095]Training (Loss: 1.6879, Grad: 3.71):   0%|          | 0/338 [00:53<?, ?it/s, cF1=0.727, nF1=0.495, rMSE=0.004, pL=0.002, tL=0.000]Training (Loss: 2.0857, Grad: 3.78):   0%|          | 0/338 [00:54<?, ?it/s, cF1=0.727, nF1=0.495, rMSE=0.004, pL=0.002, tL=0.000]Training (Loss: 2.0857, Grad: 3.78):   0%|          | 0/338 [00:54<?, ?it/s, cF1=0.739, nF1=0.507, rMSE=0.004, pL=0.010, tL=0.000]Training (Loss: 1.7954, Grad: 3.55):   0%|          | 0/338 [01:04<?, ?it/s, cF1=0.739, nF1=0.507, rMSE=0.004, pL=0.010, tL=0.000]Training (Loss: 1.7954, Grad: 3.55):   0%|          | 0/338 [01:04<?, ?it/s, cF1=0.772, nF1=0.476, rMSE=0.004, pL=0.010, tL=0.000]Training (Loss: 1.5218, Grad: 2.75):   0%|          | 0/338 [01:05<?, ?it/s, cF1=0.772, nF1=0.476, rMSE=0.004, pL=0.010, tL=0.000]Training (Loss: 1.5218, Grad: 2.75):   0%|          | 0/338 [01:05<?, ?it/s, cF1=0.746, nF1=0.502, rMSE=0.004, pL=0.006, tL=0.000]Training (Loss: 1.8879, Grad: 3.49):   0%|          | 0/338 [01:06<?, ?it/s, cF1=0.746, nF1=0.502, rMSE=0.004, pL=0.006, tL=0.000]Training (Loss: 1.8879, Grad: 3.49):   0%|          | 0/338 [01:06<?, ?it/s, cF1=0.750, nF1=0.485, rMSE=0.004, pL=0.011, tL=0.098]Training (Loss: 2.0681, Grad: 5.44):   0%|          | 0/338 [01:07<?, ?it/s, cF1=0.750, nF1=0.485, rMSE=0.004, pL=0.011, tL=0.098]Training (Loss: 2.0681, Grad: 5.44):   0%|          | 0/338 [01:07<?, ?it/s, cF1=0.753, nF1=0.485, rMSE=0.005, pL=0.010, tL=0.107]Training (Loss: 2.1826, Grad: 7.22):   0%|          | 0/338 [01:26<?, ?it/s, cF1=0.753, nF1=0.485, rMSE=0.005, pL=0.010, tL=0.107]Training (Loss: 2.1826, Grad: 7.22):   0%|          | 0/338 [01:27<?, ?it/s, cF1=0.745, nF1=0.485, rMSE=0.005, pL=0.006, tL=0.097]Training (Loss: 1.6678, Grad: 4.24):   0%|          | 0/338 [01:27<?, ?it/s, cF1=0.745, nF1=0.485, rMSE=0.005, pL=0.006, tL=0.097]Training (Loss: 1.6678, Grad: 4.24):   0%|          | 0/338 [01:27<?, ?it/s, cF1=0.743, nF1=0.484, rMSE=0.004, pL=0.007, tL=0.000]Training (Loss: 1.7223, Grad: 4.03):   0%|          | 0/338 [01:29<?, ?it/s, cF1=0.743, nF1=0.484, rMSE=0.004, pL=0.007, tL=0.000]Training (Loss: 1.7223, Grad: 4.03):   0%|          | 0/338 [01:29<?, ?it/s, cF1=0.748, nF1=0.501, rMSE=0.005, pL=0.008, tL=0.000]Training (Loss: 1.9720, Grad: 6.17):   0%|          | 0/338 [01:30<?, ?it/s, cF1=0.748, nF1=0.501, rMSE=0.005, pL=0.008, tL=0.000]Training (Loss: 1.9720, Grad: 6.17):   0%|          | 0/338 [01:30<?, ?it/s, cF1=0.722, nF1=0.504, rMSE=0.005, pL=0.005, tL=0.108]Training (Loss: 1.8259, Grad: 2.87):   0%|          | 0/338 [01:41<?, ?it/s, cF1=0.722, nF1=0.504, rMSE=0.005, pL=0.005, tL=0.108]Training (Loss: 1.8259, Grad: 2.87):   0%|          | 0/338 [01:41<?, ?it/s, cF1=0.726, nF1=0.497, rMSE=0.005, pL=0.010, tL=0.100]Training (Loss: 1.7421, Grad: 4.43):   0%|          | 0/338 [01:42<?, ?it/s, cF1=0.726, nF1=0.497, rMSE=0.005, pL=0.010, tL=0.100]Training (Loss: 1.7421, Grad: 4.43):   0%|          | 0/338 [01:42<?, ?it/s, cF1=0.737, nF1=0.493, rMSE=0.005, pL=0.013, tL=0.000]Training (Loss: 1.8560, Grad: 3.42):   0%|          | 0/338 [01:43<?, ?it/s, cF1=0.737, nF1=0.493, rMSE=0.005, pL=0.013, tL=0.000]Training (Loss: 1.8560, Grad: 3.42):   0%|          | 0/338 [01:43<?, ?it/s, cF1=0.740, nF1=0.499, rMSE=0.005, pL=0.010, tL=0.097]Training (Loss: 3.5231, Grad: 7.02):   0%|          | 0/338 [01:44<?, ?it/s, cF1=0.740, nF1=0.499, rMSE=0.005, pL=0.010, tL=0.097]Training (Loss: 3.5231, Grad: 7.02):   0%|          | 0/338 [01:44<?, ?it/s, cF1=0.749, nF1=0.491, rMSE=0.005, pL=0.012, tL=0.101]Training (Loss: 2.0548, Grad: 2.76):   0%|          | 0/338 [02:00<?, ?it/s, cF1=0.749, nF1=0.491, rMSE=0.005, pL=0.012, tL=0.101]Training (Loss: 2.0548, Grad: 2.76):   0%|          | 0/338 [02:00<?, ?it/s, cF1=0.737, nF1=0.492, rMSE=0.004, pL=0.006, tL=0.000]Training (Loss: 1.8511, Grad: 3.80):   0%|          | 0/338 [02:01<?, ?it/s, cF1=0.737, nF1=0.492, rMSE=0.004, pL=0.006, tL=0.000]Training (Loss: 1.8511, Grad: 3.80):   0%|          | 0/338 [02:01<?, ?it/s, cF1=0.727, nF1=0.491, rMSE=0.004, pL=0.006, tL=0.000]Training (Loss: 1.8244, Grad: 4.14):   0%|          | 0/338 [02:02<?, ?it/s, cF1=0.727, nF1=0.491, rMSE=0.004, pL=0.006, tL=0.000]Training (Loss: 1.8244, Grad: 4.14):   0%|          | 0/338 [02:02<?, ?it/s, cF1=0.730, nF1=0.492, rMSE=0.004, pL=0.010, tL=0.000]Training (Loss: 2.0346, Grad: 4.76):   0%|          | 0/338 [02:07<?, ?it/s, cF1=0.730, nF1=0.492, rMSE=0.004, pL=0.010, tL=0.000]Training (Loss: 2.0346, Grad: 4.76):   0%|          | 0/338 [02:07<?, ?it/s, cF1=0.733, nF1=0.485, rMSE=0.004, pL=0.010, tL=0.102]Training (Loss: 1.6224, Grad: 7.30):   0%|          | 0/338 [02:14<?, ?it/s, cF1=0.733, nF1=0.485, rMSE=0.004, pL=0.010, tL=0.102]Training (Loss: 1.6224, Grad: 7.30):   0%|          | 0/338 [02:14<?, ?it/s, cF1=0.735, nF1=0.483, rMSE=0.004, pL=0.010, tL=0.000]Training (Loss: 1.7971, Grad: 5.60):   0%|          | 0/338 [02:30<?, ?it/s, cF1=0.735, nF1=0.483, rMSE=0.004, pL=0.010, tL=0.000]Training (Loss: 1.7971, Grad: 5.60):   0%|          | 0/338 [02:30<?, ?it/s, cF1=0.732, nF1=0.483, rMSE=0.004, pL=0.007, tL=0.000]Training (Loss: 1.9242, Grad: 2.99):   0%|          | 0/338 [02:31<?, ?it/s, cF1=0.732, nF1=0.483, rMSE=0.004, pL=0.007, tL=0.000]Training (Loss: 1.9242, Grad: 2.99):   0%|          | 0/338 [02:32<?, ?it/s, cF1=0.743, nF1=0.479, rMSE=0.004, pL=0.013, tL=0.100]Training (Loss: 1.9638, Grad: 3.26):   0%|          | 0/338 [02:33<?, ?it/s, cF1=0.743, nF1=0.479, rMSE=0.004, pL=0.013, tL=0.100]Training (Loss: 1.9638, Grad: 3.26):   0%|          | 0/338 [02:33<?, ?it/s, cF1=0.736, nF1=0.476, rMSE=0.004, pL=0.004, tL=0.107]Training (Loss: 1.7693, Grad: 3.33):   0%|          | 0/338 [02:34<?, ?it/s, cF1=0.736, nF1=0.476, rMSE=0.004, pL=0.004, tL=0.107]Training (Loss: 1.7693, Grad: 3.33):   0%|          | 0/338 [02:34<?, ?it/s, cF1=0.739, nF1=0.478, rMSE=0.004, pL=0.008, tL=0.102]Training (Loss: 2.0443, Grad: 4.51):   0%|          | 0/338 [02:40<?, ?it/s, cF1=0.739, nF1=0.478, rMSE=0.004, pL=0.008, tL=0.102]Training (Loss: 2.0443, Grad: 4.51):   0%|          | 0/338 [02:40<?, ?it/s, cF1=0.738, nF1=0.478, rMSE=0.004, pL=0.008, tL=0.105]Training (Loss: 1.5476, Grad: 3.57):   0%|          | 0/338 [02:41<?, ?it/s, cF1=0.738, nF1=0.478, rMSE=0.004, pL=0.008, tL=0.105]Training (Loss: 1.5476, Grad: 3.57):   0%|          | 0/338 [02:41<?, ?it/s, cF1=0.744, nF1=0.471, rMSE=0.004, pL=0.011, tL=0.000]Training (Loss: 1.8821, Grad: 3.75):   0%|          | 0/338 [02:41<?, ?it/s, cF1=0.744, nF1=0.471, rMSE=0.004, pL=0.011, tL=0.000]Training (Loss: 1.8821, Grad: 3.75):   0%|          | 0/338 [02:41<?, ?it/s, cF1=0.741, nF1=0.470, rMSE=0.004, pL=0.008, tL=0.000]Training (Loss: 1.6513, Grad: 3.12):   0%|          | 0/338 [02:55<?, ?it/s, cF1=0.741, nF1=0.470, rMSE=0.004, pL=0.008, tL=0.000]Training (Loss: 1.6513, Grad: 3.12):   0%|          | 0/338 [02:55<?, ?it/s, cF1=0.734, nF1=0.467, rMSE=0.004, pL=0.004, tL=0.000]Training (Loss: 1.9545, Grad: 3.53):   0%|          | 0/338 [03:02<?, ?it/s, cF1=0.734, nF1=0.467, rMSE=0.004, pL=0.004, tL=0.000]Training (Loss: 1.9545, Grad: 3.53):   0%|          | 0/338 [03:02<?, ?it/s, cF1=0.727, nF1=0.469, rMSE=0.004, pL=0.005, tL=0.100]Training (Loss: 1.9733, Grad: 4.25):   0%|          | 0/338 [03:02<?, ?it/s, cF1=0.727, nF1=0.469, rMSE=0.004, pL=0.005, tL=0.100]Training (Loss: 1.9733, Grad: 4.25):   0%|          | 0/338 [03:02<?, ?it/s, cF1=0.725, nF1=0.471, rMSE=0.004, pL=0.008, tL=0.105]Training (Loss: 2.0392, Grad: nan):   0%|          | 0/338 [03:04<?, ?it/s, cF1=0.725, nF1=0.471, rMSE=0.004, pL=0.008, tL=0.105] Training (Loss: 2.0392, Grad: nan):   0%|          | 0/338 [03:04<?, ?it/s, cF1=0.723, nF1=0.473, rMSE=0.004, pL=0.009, tL=0.148]Training (Loss: 2.2955, Grad: 4.35):   0%|          | 0/338 [03:10<?, ?it/s, cF1=0.723, nF1=0.473, rMSE=0.004, pL=0.009, tL=0.148]Training (Loss: 2.2955, Grad: 4.35):   0%|          | 0/338 [03:10<?, ?it/s, cF1=0.721, nF1=0.474, rMSE=0.004, pL=0.008, tL=0.102]Training (Loss: 1.8223, Grad: 5.45):   0%|          | 0/338 [03:22<?, ?it/s, cF1=0.721, nF1=0.474, rMSE=0.004, pL=0.008, tL=0.102]Training (Loss: 1.8223, Grad: 5.45):   0%|          | 0/338 [03:22<?, ?it/s, cF1=0.718, nF1=0.474, rMSE=0.004, pL=0.006, tL=0.000]Training (Loss: 1.8925, Grad: 3.69):   0%|          | 0/338 [03:23<?, ?it/s, cF1=0.718, nF1=0.474, rMSE=0.004, pL=0.006, tL=0.000]Training (Loss: 1.8925, Grad: 3.69):   0%|          | 0/338 [03:23<?, ?it/s, cF1=0.723, nF1=0.479, rMSE=0.004, pL=0.013, tL=0.100]Training (Loss: 1.9107, Grad: 5.45):   0%|          | 0/338 [03:24<?, ?it/s, cF1=0.723, nF1=0.479, rMSE=0.004, pL=0.013, tL=0.100]Training (Loss: 1.9107, Grad: 5.45):   0%|          | 0/338 [03:24<?, ?it/s, cF1=0.718, nF1=0.478, rMSE=0.005, pL=0.006, tL=0.104]Training (Loss: 1.9586, Grad: 11.48):   0%|          | 0/338 [03:32<?, ?it/s, cF1=0.718, nF1=0.478, rMSE=0.005, pL=0.006, tL=0.104]Training (Loss: 1.9586, Grad: 11.48):   0%|          | 0/338 [03:32<?, ?it/s, cF1=0.720, nF1=0.477, rMSE=0.005, pL=0.010, tL=0.098]Training (Loss: 1.5816, Grad: 4.38):   0%|          | 0/338 [03:32<?, ?it/s, cF1=0.720, nF1=0.477, rMSE=0.005, pL=0.010, tL=0.098] Training (Loss: 1.5816, Grad: 4.38):   0%|          | 0/338 [03:32<?, ?it/s, cF1=0.716, nF1=0.476, rMSE=0.005, pL=0.004, tL=0.000]Training (Loss: 1.5845, Grad: 3.35):   0%|          | 0/338 [03:37<?, ?it/s, cF1=0.716, nF1=0.476, rMSE=0.005, pL=0.004, tL=0.000]Training (Loss: 1.5845, Grad: 3.35):   0%|          | 0/338 [03:37<?, ?it/s, cF1=0.711, nF1=0.474, rMSE=0.005, pL=0.006, tL=0.000]Training (Loss: 1.8420, Grad: 3.92):   0%|          | 0/338 [03:38<?, ?it/s, cF1=0.711, nF1=0.474, rMSE=0.005, pL=0.006, tL=0.000]Training (Loss: 1.8420, Grad: 3.92):   0%|          | 0/338 [03:38<?, ?it/s, cF1=0.713, nF1=0.473, rMSE=0.005, pL=0.008, tL=0.104]Training (Loss: 1.8583, Grad: 4.30):   0%|          | 0/338 [03:48<?, ?it/s, cF1=0.713, nF1=0.473, rMSE=0.005, pL=0.008, tL=0.104]Training (Loss: 1.8583, Grad: 4.30):   0%|          | 0/338 [03:48<?, ?it/s, cF1=0.712, nF1=0.470, rMSE=0.005, pL=0.007, tL=0.098]Training (Loss: 1.9720, Grad: 6.38):   0%|          | 0/338 [03:55<?, ?it/s, cF1=0.712, nF1=0.470, rMSE=0.005, pL=0.007, tL=0.098]Training (Loss: 1.9720, Grad: 6.38):   0%|          | 0/338 [03:55<?, ?it/s, cF1=0.712, nF1=0.468, rMSE=0.005, pL=0.007, tL=0.090]Training (Loss: 2.0930, Grad: 4.20):   0%|          | 0/338 [04:05<?, ?it/s, cF1=0.712, nF1=0.468, rMSE=0.005, pL=0.007, tL=0.090]Training (Loss: 2.0930, Grad: 4.20):   0%|          | 0/338 [04:05<?, ?it/s, cF1=0.713, nF1=0.472, rMSE=0.005, pL=0.010, tL=0.088]Training (Loss: 2.0930, Grad: 4.20):  13%|█▎        | 43/338 [04:05<28:07,  5.72s/it, cF1=0.713, nF1=0.472, rMSE=0.005, pL=0.010, tL=0.088]Training (Loss: 2.9654, Grad: 5.57):  13%|█▎        | 43/338 [04:06<28:07,  5.72s/it, cF1=0.713, nF1=0.472, rMSE=0.005, pL=0.010, tL=0.088]Training (Loss: 2.9654, Grad: 5.57):  13%|█▎        | 43/338 [04:06<28:07,  5.72s/it, cF1=0.709, nF1=0.470, rMSE=0.005, pL=0.008, tL=0.000]Training (Loss: 1.7607, Grad: 4.61):  13%|█▎        | 43/338 [04:07<28:07,  5.72s/it, cF1=0.709, nF1=0.470, rMSE=0.005, pL=0.008, tL=0.000]Training (Loss: 1.7607, Grad: 4.61):  13%|█▎        | 43/338 [04:07<28:07,  5.72s/it, cF1=0.703, nF1=0.469, rMSE=0.005, pL=0.004, tL=0.000]Training (Loss: 1.8060, Grad: 3.40):  13%|█▎        | 43/338 [04:17<28:07,  5.72s/it, cF1=0.703, nF1=0.469, rMSE=0.005, pL=0.004, tL=0.000]Training (Loss: 1.8060, Grad: 3.40):  13%|█▎        | 43/338 [04:17<28:07,  5.72s/it, cF1=0.702, nF1=0.467, rMSE=0.005, pL=0.008, tL=0.000]Training (Loss: 1.8060, Grad: 3.40):  13%|█▎        | 43/338 [04:20<28:07,  5.72s/it, cF1=0.702, nF1=0.467, rMSE=0.005, pL=0.008, tL=0.000]Training (Loss: 2.0174, Grad: 3.46):  13%|█▎        | 43/338 [04:27<28:07,  5.72s/it, cF1=0.702, nF1=0.467, rMSE=0.005, pL=0.008, tL=0.000]Training (Loss: 2.0174, Grad: 3.46):  13%|█▎        | 43/338 [04:27<28:07,  5.72s/it, cF1=0.703, nF1=0.464, rMSE=0.005, pL=0.011, tL=0.095]Training (Loss: 1.6962, Grad: 4.64):  13%|█▎        | 43/338 [04:29<28:07,  5.72s/it, cF1=0.703, nF1=0.464, rMSE=0.005, pL=0.011, tL=0.095]Training (Loss: 1.6962, Grad: 4.64):  13%|█▎        | 43/338 [04:29<28:07,  5.72s/it, cF1=0.702, nF1=0.463, rMSE=0.005, pL=0.008, tL=0.000]Training (Loss: 1.4758, Grad: 3.10):  13%|█▎        | 43/338 [04:31<28:07,  5.72s/it, cF1=0.702, nF1=0.463, rMSE=0.005, pL=0.008, tL=0.000]Training (Loss: 1.4758, Grad: 3.10):  13%|█▎        | 43/338 [04:31<28:07,  5.72s/it, cF1=0.699, nF1=0.462, rMSE=0.005, pL=0.006, tL=0.000]Training (Loss: 1.6349, Grad: 5.03):  13%|█▎        | 43/338 [04:32<28:07,  5.72s/it, cF1=0.699, nF1=0.462, rMSE=0.005, pL=0.006, tL=0.000]Training (Loss: 1.6349, Grad: 5.03):  13%|█▎        | 43/338 [04:32<28:07,  5.72s/it, cF1=0.694, nF1=0.460, rMSE=0.005, pL=0.004, tL=0.000]