nohup: ignoring input
================================================================================
CASCADE FAILURE PREDICTION - TRAINING SCRIPT (IMPROVED)
================================================================================

Configuration:
  Data directory: data
  Output directory: checkpoints
  Batch size: 8
  Epochs: 100
  Learning rate: 0.0001
  Device: cuda
  Gradient clipping: 1.0
  Mixed precision: True
  Resume training: False

Loading datasets...
Indexing scenarios from: data/train
Physics normalization: base_mva=100.0, base_frequency=60.0
Scanning 2700 files for cascade labels...
Indexed 2700 scenarios.
  Cascade scenarios: 905 (33.5%)
  Normal scenarios: 1795 (66.5%)
Ultra-memory-efficient mode: Loading 1 file per sample.
Indexing scenarios from: data/val
Physics normalization: base_mva=100.0, base_frequency=60.0
Scanning 491 files for cascade labels...
Indexed 491 scenarios.
  Cascade scenarios: 170 (34.6%)
  Normal scenarios: 321 (65.4%)
Ultra-memory-efficient mode: Loading 1 file per sample.
  Training samples: 2700
  Validation samples: 491
  Mode: full_sequence (utilizing 3-layer LSTM for temporal modeling)

Computing sample weights for balanced sampling...
  Positive samples: 905 (33.5%)
  Negative samples: 1795 (66.5%)
  Calculated weights -> Pos: 2.98, Neg: 1.50 (creates ~1:1 batch balance)

Initializing model...
  Total parameters: 806,463
  Trainable parameters: 806,463

================================================================================
STARTING DYNAMIC LOSS WEIGHT CALIBRATION
================================================================================
Running loss calibration for 20 batches...
  Average raw loss components (unweighted):
    frequency      :   1.422845
    powerflow      :   6.431766
    prediction     :   0.081649
    reactive       :   0.706939
    risk           :   0.122412
    temperature    :   0.094321
    timing         :   0.075100
    voltage        :   0.664456
================================================================================
CALIBRATION COMPLETE
================================================================================

Balancing loss weights dynamically...
  Target Magnitude (from prediction loss): 0.0816

  Final Loss Weights (Fully Dynamic):
  Component       | Raw Loss     | Final Lambda | Initial Weighted Loss
  --------------- | ------------ | ------------ | --------------------
  Timing          | 0.0751       | 1.0872       |       0.0816
  Powerflow       | 6.4318       | 0.0127       |       0.0816
  Temperature     | 0.0943       | 0.8656       |       0.0816
  Reactive        | 0.7069       | 0.1155       |       0.0816
  Voltage         | 0.6645       | 0.1229       |       0.0816
  Frequency       | 1.4228       | 0.0574       |       0.0816
  Risk            | 0.1224       | 0.6670       |       0.0816

✓ PhysicsInformedLoss initialized with FINAL dynamic weights.

================================================================================
STARTING TRAINING
================================================================================


Epoch 1/100
--------------------------------------------------------------------------------
Training:   0%|          | 0/338 [00:00<?, ?it/s]
================================================================================
MODEL OUTPUT VALIDATION (First Batch)
================================================================================
Checking required outputs for loss calculation...
  ✓ failure_probability: shape (8, 118, 1) (Matches expected)
  ✓ voltages: shape (8, 118, 1) (Matches expected)
  ✓ angles: shape (8, 118, 1) (Matches expected)
  ✓ line_flows: shape (8, 686, 1) (Matches expected)
  ✓ frequency: shape (8, 1, 1) (Matches expected)
  ✓ risk_scores: shape (8, 118, 7) (Matches expected)
  ✓ cascade_timing: shape (8, 118, 1) (Matches expected)
  ✓ temperature: shape (8, 118, 1) (Matches expected)

Checking other model outputs...
  ✓ reactive_flows: shape (8, 686, 1) (Matches expected)

Temporal sequence detected: B=8, T=35, N=118
  ✓ 3-layer LSTM IS BEING UTILIZED.
================================================================================

Training (Loss: 0.6108, Grad: 0.99):   0%|          | 0/338 [00:05<?, ?it/s]Training (Loss: 0.6108, Grad: 0.99):   0%|          | 0/338 [00:06<?, ?it/s, cF1=0.769, nF1=0.445, rMSE=0.131, pL=0.086]Training (Loss: 0.6108, Grad: 0.99):   0%|          | 1/338 [00:06<34:19,  6.11s/it, cF1=0.769, nF1=0.445, rMSE=0.131, pL=0.086]Training (Loss: 0.5012, Grad: 1.13):   0%|          | 1/338 [00:12<34:19,  6.11s/it, cF1=0.769, nF1=0.445, rMSE=0.131, pL=0.086]Training (Loss: 0.5012, Grad: 1.13):   0%|          | 1/338 [00:12<34:19,  6.11s/it, cF1=0.545, nF1=0.266, rMSE=0.116, pL=0.058]Training (Loss: 0.5012, Grad: 1.13):   1%|          | 2/338 [00:12<35:40,  6.37s/it, cF1=0.545, nF1=0.266, rMSE=0.116, pL=0.058]Training (Loss: 0.4930, Grad: 1.03):   1%|          | 2/338 [00:19<35:40,  6.37s/it, cF1=0.545, nF1=0.266, rMSE=0.116, pL=0.058]Training (Loss: 0.4930, Grad: 1.03):   1%|          | 2/338 [00:19<35:40,  6.37s/it, cF1=0.500, nF1=0.212, rMSE=0.111, pL=0.060]Training (Loss: 0.4930, Grad: 1.03):   1%|          | 3/338 [00:19<36:34,  6.55s/it, cF1=0.500, nF1=0.212, rMSE=0.111, pL=0.060]Training (Loss: 0.5342, Grad: 1.06):   1%|          | 3/338 [00:26<36:34,  6.55s/it, cF1=0.500, nF1=0.212, rMSE=0.111, pL=0.060]Training (Loss: 0.5342, Grad: 1.06):   1%|          | 3/338 [00:26<36:34,  6.55s/it, cF1=0.512, nF1=0.216, rMSE=0.124, pL=0.062]Training (Loss: 0.5342, Grad: 1.06):   1%|          | 4/338 [00:26<38:09,  6.86s/it, cF1=0.512, nF1=0.216, rMSE=0.124, pL=0.062]Training (Loss: 0.5500, Grad: 1.00):   1%|          | 4/338 [00:30<38:09,  6.86s/it, cF1=0.512, nF1=0.216, rMSE=0.124, pL=0.062]Training (Loss: 0.5500, Grad: 1.00):   1%|          | 4/338 [00:30<38:09,  6.86s/it, cF1=0.545, nF1=0.237, rMSE=0.124, pL=0.072]Training (Loss: 0.5500, Grad: 1.00):   1%|▏         | 5/338 [00:30<32:33,  5.87s/it, cF1=0.545, nF1=0.237, rMSE=0.124, pL=0.072]Training (Loss: 0.5220, Grad: 0.88):   1%|▏         | 5/338 [00:38<32:33,  5.87s/it, cF1=0.545, nF1=0.237, rMSE=0.124, pL=0.072]Training (Loss: 0.5220, Grad: 0.88):   1%|▏         | 5/338 [00:38<32:33,  5.87s/it, cF1=0.567, nF1=0.238, rMSE=0.121, pL=0.066]Training (Loss: 0.5220, Grad: 0.88):   2%|▏         | 6/338 [00:38<36:11,  6.54s/it, cF1=0.567, nF1=0.238, rMSE=0.121, pL=0.066]Training (Loss: 0.5259, Grad: 0.85):   2%|▏         | 6/338 [00:46<36:11,  6.54s/it, cF1=0.567, nF1=0.238, rMSE=0.121, pL=0.066]Training (Loss: 0.5259, Grad: 0.85):   2%|▏         | 6/338 [00:46<36:11,  6.54s/it, cF1=0.582, nF1=0.244, rMSE=0.120, pL=0.066]Training (Loss: 0.5259, Grad: 0.85):   2%|▏         | 7/338 [00:46<38:11,  6.92s/it, cF1=0.582, nF1=0.244, rMSE=0.120, pL=0.066]Training (Loss: 0.5028, Grad: 0.77):   2%|▏         | 7/338 [00:52<38:11,  6.92s/it, cF1=0.582, nF1=0.244, rMSE=0.120, pL=0.066]Training (Loss: 0.5028, Grad: 0.77):   2%|▏         | 7/338 [00:52<38:11,  6.92s/it, cF1=0.609, nF1=0.242, rMSE=0.118, pL=0.060]Training (Loss: 0.5028, Grad: 0.77):   2%|▏         | 8/338 [00:52<36:01,  6.55s/it, cF1=0.609, nF1=0.242, rMSE=0.118, pL=0.060]Training (Loss: 0.4897, Grad: 0.72):   2%|▏         | 8/338 [00:59<36:01,  6.55s/it, cF1=0.609, nF1=0.242, rMSE=0.118, pL=0.060]Training (Loss: 0.4897, Grad: 0.72):   2%|▏         | 8/338 [00:59<36:01,  6.55s/it, cF1=0.615, nF1=0.238, rMSE=0.117, pL=0.059]Training (Loss: 0.4897, Grad: 0.72):   3%|▎         | 9/338 [00:59<37:07,  6.77s/it, cF1=0.615, nF1=0.238, rMSE=0.117, pL=0.059]Training (Loss: 0.4206, Grad: 0.69):   3%|▎         | 9/338 [01:06<37:07,  6.77s/it, cF1=0.615, nF1=0.238, rMSE=0.117, pL=0.059]Training (Loss: 0.4206, Grad: 0.69):   3%|▎         | 9/338 [01:06<37:07,  6.77s/it, cF1=0.584, nF1=0.225, rMSE=0.116, pL=0.048]Training (Loss: 0.4206, Grad: 0.69):   3%|▎         | 10/338 [01:06<37:18,  6.82s/it, cF1=0.584, nF1=0.225, rMSE=0.116, pL=0.048]Training (Loss: 0.5091, Grad: 0.59):   3%|▎         | 10/338 [01:14<37:18,  6.82s/it, cF1=0.584, nF1=0.225, rMSE=0.116, pL=0.048]Training (Loss: 0.5091, Grad: 0.59):   3%|▎         | 10/338 [01:14<37:18,  6.82s/it, cF1=0.614, nF1=0.231, rMSE=0.115, pL=0.063]Training (Loss: 0.5091, Grad: 0.59):   3%|▎         | 11/338 [01:14<38:55,  7.14s/it, cF1=0.614, nF1=0.231, rMSE=0.115, pL=0.063]Training (Loss: 0.4647, Grad: 0.62):   3%|▎         | 11/338 [01:18<38:55,  7.14s/it, cF1=0.614, nF1=0.231, rMSE=0.115, pL=0.063]Training (Loss: 0.4647, Grad: 0.62):   3%|▎         | 11/338 [01:18<38:55,  7.14s/it, cF1=0.609, nF1=0.231, rMSE=0.113, pL=0.056]Training (Loss: 0.4647, Grad: 0.62):   4%|▎         | 12/338 [01:18<33:23,  6.15s/it, cF1=0.609, nF1=0.231, rMSE=0.113, pL=0.056]Training (Loss: 0.5053, Grad: 0.54):   4%|▎         | 12/338 [01:24<33:23,  6.15s/it, cF1=0.609, nF1=0.231, rMSE=0.113, pL=0.056]Training (Loss: 0.5053, Grad: 0.54):   4%|▎         | 12/338 [01:24<33:23,  6.15s/it, cF1=0.623, nF1=0.236, rMSE=0.113, pL=0.063]Training (Loss: 0.5053, Grad: 0.54):   4%|▍         | 13/338 [01:24<34:27,  6.36s/it, cF1=0.623, nF1=0.236, rMSE=0.113, pL=0.063]Training (Loss: 0.4958, Grad: 0.47):   4%|▍         | 13/338 [01:30<34:27,  6.36s/it, cF1=0.623, nF1=0.236, rMSE=0.113, pL=0.063]Training (Loss: 0.4958, Grad: 0.47):   4%|▍         | 13/338 [01:30<34:27,  6.36s/it, cF1=0.634, nF1=0.239, rMSE=0.112, pL=0.061]Training (Loss: 0.4958, Grad: 0.47):   4%|▍         | 14/338 [01:30<33:10,  6.14s/it, cF1=0.634, nF1=0.239, rMSE=0.112, pL=0.061]Training (Loss: 0.5016, Grad: 0.57):   4%|▍         | 14/338 [01:34<33:10,  6.14s/it, cF1=0.634, nF1=0.239, rMSE=0.112, pL=0.061]Training (Loss: 0.5016, Grad: 0.57):   4%|▍         | 14/338 [01:34<33:10,  6.14s/it, cF1=0.644, nF1=0.251, rMSE=0.111, pL=0.076]Training (Loss: 0.5016, Grad: 0.57):   4%|▍         | 15/338 [01:34<29:13,  5.43s/it, cF1=0.644, nF1=0.251, rMSE=0.111, pL=0.076]Training (Loss: 0.4703, Grad: 0.41):   4%|▍         | 15/338 [01:40<29:13,  5.43s/it, cF1=0.644, nF1=0.251, rMSE=0.111, pL=0.076]Training (Loss: 0.4703, Grad: 0.41):   4%|▍         | 15/338 [01:40<29:13,  5.43s/it, cF1=0.646, nF1=0.256, rMSE=0.111, pL=0.070]Training (Loss: 0.4703, Grad: 0.41):   5%|▍         | 16/338 [01:40<30:43,  5.73s/it, cF1=0.646, nF1=0.256, rMSE=0.111, pL=0.070]Training (Loss: 0.4183, Grad: 0.43):   5%|▍         | 16/338 [01:47<30:43,  5.73s/it, cF1=0.646, nF1=0.256, rMSE=0.111, pL=0.070]Training (Loss: 0.4183, Grad: 0.43):   5%|▍         | 16/338 [01:47<30:43,  5.73s/it, cF1=0.633, nF1=0.248, rMSE=0.111, pL=0.044]Training (Loss: 0.4183, Grad: 0.43):   5%|▌         | 17/338 [01:47<31:53,  5.96s/it, cF1=0.633, nF1=0.248, rMSE=0.111, pL=0.044]Training (Loss: 0.5068, Grad: 0.44):   5%|▌         | 17/338 [01:51<31:53,  5.96s/it, cF1=0.633, nF1=0.248, rMSE=0.111, pL=0.044]Training (Loss: 0.5068, Grad: 0.44):   5%|▌         | 17/338 [01:51<31:53,  5.96s/it, cF1=0.654, nF1=0.255, rMSE=0.111, pL=0.075]Training (Loss: 0.5068, Grad: 0.44):   5%|▌         | 18/338 [01:51<28:53,  5.42s/it, cF1=0.654, nF1=0.255, rMSE=0.111, pL=0.075]Training (Loss: 0.4718, Grad: 0.33):   5%|▌         | 18/338 [01:56<28:53,  5.42s/it, cF1=0.654, nF1=0.255, rMSE=0.111, pL=0.075]Training (Loss: 0.4718, Grad: 0.33):   5%|▌         | 18/338 [01:56<28:53,  5.42s/it, cF1=0.661, nF1=0.256, rMSE=0.111, pL=0.062]Training (Loss: 0.4718, Grad: 0.33):   6%|▌         | 19/338 [01:56<28:49,  5.42s/it, cF1=0.661, nF1=0.256, rMSE=0.111, pL=0.062]Training (Loss: 0.4917, Grad: 0.35):   6%|▌         | 19/338 [02:02<28:49,  5.42s/it, cF1=0.661, nF1=0.256, rMSE=0.111, pL=0.062]Training (Loss: 0.4917, Grad: 0.35):   6%|▌         | 19/338 [02:02<28:49,  5.42s/it, cF1=0.661, nF1=0.260, rMSE=0.111, pL=0.072]Training (Loss: 0.4917, Grad: 0.35):   6%|▌         | 20/338 [02:02<29:04,  5.49s/it, cF1=0.661, nF1=0.260, rMSE=0.111, pL=0.072]Training (Loss: 0.4489, Grad: 0.34):   6%|▌         | 20/338 [02:09<29:04,  5.49s/it, cF1=0.661, nF1=0.260, rMSE=0.111, pL=0.072]Training (Loss: 0.4489, Grad: 0.34):   6%|▌         | 20/338 [02:09<29:04,  5.49s/it, cF1=0.661, nF1=0.257, rMSE=0.111, pL=0.051]Training (Loss: 0.4489, Grad: 0.34):   6%|▌         | 21/338 [02:09<32:07,  6.08s/it, cF1=0.661, nF1=0.257, rMSE=0.111, pL=0.051]Training (Loss: 0.4905, Grad: 0.39):   6%|▌         | 21/338 [02:15<32:07,  6.08s/it, cF1=0.661, nF1=0.257, rMSE=0.111, pL=0.051]Training (Loss: 0.4905, Grad: 0.39):   6%|▌         | 21/338 [02:15<32:07,  6.08s/it, cF1=0.672, nF1=0.264, rMSE=0.110, pL=0.080]Training (Loss: 0.4905, Grad: 0.39):   7%|▋         | 22/338 [02:15<30:53,  5.87s/it, cF1=0.672, nF1=0.264, rMSE=0.110, pL=0.080]Training (Loss: 0.4840, Grad: 0.35):   7%|▋         | 22/338 [02:20<30:53,  5.87s/it, cF1=0.672, nF1=0.264, rMSE=0.110, pL=0.080]Training (Loss: 0.4840, Grad: 0.35):   7%|▋         | 22/338 [02:20<30:53,  5.87s/it, cF1=0.676, nF1=0.268, rMSE=0.110, pL=0.069]Training (Loss: 0.4840, Grad: 0.35):   7%|▋         | 23/338 [02:20<29:54,  5.70s/it, cF1=0.676, nF1=0.268, rMSE=0.110, pL=0.069]Training (Loss: 0.4466, Grad: 0.31):   7%|▋         | 23/338 [02:26<29:54,  5.70s/it, cF1=0.676, nF1=0.268, rMSE=0.110, pL=0.069]Training (Loss: 0.4466, Grad: 0.31):   7%|▋         | 23/338 [02:26<29:54,  5.70s/it, cF1=0.676, nF1=0.267, rMSE=0.109, pL=0.058]Training (Loss: 0.4466, Grad: 0.31):   7%|▋         | 24/338 [02:26<30:28,  5.82s/it, cF1=0.676, nF1=0.267, rMSE=0.109, pL=0.058]Training (Loss: 0.3961, Grad: 0.35):   7%|▋         | 24/338 [02:32<30:28,  5.82s/it, cF1=0.676, nF1=0.267, rMSE=0.109, pL=0.058]Training (Loss: 0.3961, Grad: 0.35):   7%|▋         | 24/338 [02:32<30:28,  5.82s/it, cF1=0.667, nF1=0.262, rMSE=0.108, pL=0.043]Training (Loss: 0.3961, Grad: 0.35):   7%|▋         | 25/338 [02:32<30:46,  5.90s/it, cF1=0.667, nF1=0.262, rMSE=0.108, pL=0.043]Training (Loss: 0.4669, Grad: 0.31):   7%|▋         | 25/338 [02:37<30:46,  5.90s/it, cF1=0.667, nF1=0.262, rMSE=0.108, pL=0.043]Training (Loss: 0.4669, Grad: 0.31):   7%|▋         | 25/338 [02:37<30:46,  5.90s/it, cF1=0.671, nF1=0.263, rMSE=0.108, pL=0.061]Training (Loss: 0.4669, Grad: 0.31):   8%|▊         | 26/338 [02:37<28:12,  5.43s/it, cF1=0.671, nF1=0.263, rMSE=0.108, pL=0.061]Training (Loss: 0.5013, Grad: 0.36):   8%|▊         | 26/338 [02:42<28:12,  5.43s/it, cF1=0.671, nF1=0.263, rMSE=0.108, pL=0.061]Training (Loss: 0.5013, Grad: 0.36):   8%|▊         | 26/338 [02:42<28:12,  5.43s/it, cF1=0.679, nF1=0.269, rMSE=0.107, pL=0.078]Training (Loss: 0.5013, Grad: 0.36):   8%|▊         | 27/338 [02:42<28:33,  5.51s/it, cF1=0.679, nF1=0.269, rMSE=0.107, pL=0.078]Training (Loss: 0.4660, Grad: 0.28):   8%|▊         | 27/338 [02:51<28:33,  5.51s/it, cF1=0.679, nF1=0.269, rMSE=0.107, pL=0.078]Training (Loss: 0.4660, Grad: 0.28):   8%|▊         | 27/338 [02:51<28:33,  5.51s/it, cF1=0.682, nF1=0.270, rMSE=0.107, pL=0.065]Training (Loss: 0.4660, Grad: 0.28):   8%|▊         | 28/338 [02:51<32:44,  6.34s/it, cF1=0.682, nF1=0.270, rMSE=0.107, pL=0.065]Training (Loss: 0.4717, Grad: 0.32):   8%|▊         | 28/338 [02:57<32:44,  6.34s/it, cF1=0.682, nF1=0.270, rMSE=0.107, pL=0.065]Training (Loss: 0.4717, Grad: 0.32):   8%|▊         | 28/338 [02:57<32:44,  6.34s/it, cF1=0.686, nF1=0.272, rMSE=0.106, pL=0.070]Training (Loss: 0.4717, Grad: 0.32):   9%|▊         | 29/338 [02:57<33:14,  6.45s/it, cF1=0.686, nF1=0.272, rMSE=0.106, pL=0.070]Training (Loss: 0.4252, Grad: 0.32):   9%|▊         | 29/338 [03:04<33:14,  6.45s/it, cF1=0.686, nF1=0.272, rMSE=0.106, pL=0.070]Training (Loss: 0.4252, Grad: 0.32):   9%|▊         | 29/338 [03:04<33:14,  6.45s/it, cF1=0.685, nF1=0.271, rMSE=0.106, pL=0.056]Training (Loss: 0.4252, Grad: 0.32):   9%|▉         | 30/338 [03:04<32:58,  6.42s/it, cF1=0.685, nF1=0.271, rMSE=0.106, pL=0.056]Training (Loss: 0.4297, Grad: 0.29):   9%|▉         | 30/338 [03:10<32:58,  6.42s/it, cF1=0.685, nF1=0.271, rMSE=0.106, pL=0.056]Training (Loss: 0.4297, Grad: 0.29):   9%|▉         | 30/338 [03:10<32:58,  6.42s/it, cF1=0.684, nF1=0.269, rMSE=0.105, pL=0.052]Training (Loss: 0.4297, Grad: 0.29):   9%|▉         | 31/338 [03:10<33:01,  6.46s/it, cF1=0.684, nF1=0.269, rMSE=0.105, pL=0.052]Training (Loss: 0.4393, Grad: 0.30):   9%|▉         | 31/338 [03:15<33:01,  6.46s/it, cF1=0.684, nF1=0.269, rMSE=0.105, pL=0.052]Training (Loss: 0.4393, Grad: 0.30):   9%|▉         | 31/338 [03:15<33:01,  6.46s/it, cF1=0.684, nF1=0.269, rMSE=0.105, pL=0.057]Training (Loss: 0.4393, Grad: 0.30):   9%|▉         | 32/338 [03:15<30:55,  6.06s/it, cF1=0.684, nF1=0.269, rMSE=0.105, pL=0.057]Training (Loss: 0.3957, Grad: 0.36):   9%|▉         | 32/338 [03:21<30:55,  6.06s/it, cF1=0.684, nF1=0.269, rMSE=0.105, pL=0.057]Training (Loss: 0.3957, Grad: 0.36):   9%|▉         | 32/338 [03:21<30:55,  6.06s/it, cF1=0.677, nF1=0.265, rMSE=0.104, pL=0.043]Training (Loss: 0.3957, Grad: 0.36):  10%|▉         | 33/338 [03:21<29:48,  5.86s/it, cF1=0.677, nF1=0.265, rMSE=0.104, pL=0.043]Training (Loss: 0.4771, Grad: 0.29):  10%|▉         | 33/338 [03:27<29:48,  5.86s/it, cF1=0.677, nF1=0.265, rMSE=0.104, pL=0.043]Training (Loss: 0.4771, Grad: 0.29):  10%|▉         | 33/338 [03:27<29:48,  5.86s/it, cF1=0.683, nF1=0.267, rMSE=0.103, pL=0.070]Training (Loss: 0.4771, Grad: 0.29):  10%|█         | 34/338 [03:27<30:18,  5.98s/it, cF1=0.683, nF1=0.267, rMSE=0.103, pL=0.070]Training (Loss: 0.3627, Grad: 0.40):  10%|█         | 34/338 [03:34<30:18,  5.98s/it, cF1=0.683, nF1=0.267, rMSE=0.103, pL=0.070]Training (Loss: 0.3627, Grad: 0.40):  10%|█         | 34/338 [03:34<30:18,  5.98s/it, cF1=0.673, nF1=0.262, rMSE=0.103, pL=0.035]Training (Loss: 0.3627, Grad: 0.40):  10%|█         | 35/338 [03:34<32:23,  6.41s/it, cF1=0.673, nF1=0.262, rMSE=0.103, pL=0.035]Training (Loss: 0.3794, Grad: 0.38):  10%|█         | 35/338 [03:42<32:23,  6.41s/it, cF1=0.673, nF1=0.262, rMSE=0.103, pL=0.035]Training (Loss: 0.3794, Grad: 0.38):  10%|█         | 35/338 [03:42<32:23,  6.41s/it, cF1=0.667, nF1=0.258, rMSE=0.102, pL=0.042]Training (Loss: 0.3794, Grad: 0.38):  11%|█         | 36/338 [03:42<33:31,  6.66s/it, cF1=0.667, nF1=0.258, rMSE=0.102, pL=0.042]Training (Loss: 0.4401, Grad: 0.23):  11%|█         | 36/338 [03:48<33:31,  6.66s/it, cF1=0.667, nF1=0.258, rMSE=0.102, pL=0.042]Training (Loss: 0.4401, Grad: 0.23):  11%|█         | 36/338 [03:48<33:31,  6.66s/it, cF1=0.670, nF1=0.258, rMSE=0.101, pL=0.058]Training (Loss: 0.4401, Grad: 0.23):  11%|█         | 37/338 [03:48<32:40,  6.51s/it, cF1=0.670, nF1=0.258, rMSE=0.101, pL=0.058]Training (Loss: 0.4834, Grad: 0.26):  11%|█         | 37/338 [03:53<32:40,  6.51s/it, cF1=0.670, nF1=0.258, rMSE=0.101, pL=0.058]Training (Loss: 0.4834, Grad: 0.26):  11%|█         | 37/338 [03:53<32:40,  6.51s/it, cF1=0.675, nF1=0.260, rMSE=0.101, pL=0.068]Training (Loss: 0.4834, Grad: 0.26):  11%|█         | 38/338 [03:53<30:19,  6.06s/it, cF1=0.675, nF1=0.260, rMSE=0.101, pL=0.068]Training (Loss: 0.4640, Grad: 0.27):  11%|█         | 38/338 [03:58<30:19,  6.06s/it, cF1=0.675, nF1=0.260, rMSE=0.101, pL=0.068]Training (Loss: 0.4640, Grad: 0.27):  11%|█         | 38/338 [03:58<30:19,  6.06s/it, cF1=0.681, nF1=0.262, rMSE=0.100, pL=0.069]Training (Loss: 0.4640, Grad: 0.27):  12%|█▏        | 39/338 [03:58<28:22,  5.69s/it, cF1=0.681, nF1=0.262, rMSE=0.100, pL=0.069]Training (Loss: 0.4316, Grad: 0.25):  12%|█▏        | 39/338 [04:06<28:22,  5.69s/it, cF1=0.681, nF1=0.262, rMSE=0.100, pL=0.069]Training (Loss: 0.4316, Grad: 0.25):  12%|█▏        | 39/338 [04:06<28:22,  5.69s/it, cF1=0.680, nF1=0.262, rMSE=0.100, pL=0.060]Training (Loss: 0.4316, Grad: 0.25):  12%|█▏        | 40/338 [04:06<31:53,  6.42s/it, cF1=0.680, nF1=0.262, rMSE=0.100, pL=0.060]Training (Loss: 0.4812, Grad: 0.31):  12%|█▏        | 40/338 [04:14<31:53,  6.42s/it, cF1=0.680, nF1=0.262, rMSE=0.100, pL=0.060]Training (Loss: 0.4812, Grad: 0.31):  12%|█▏        | 40/338 [04:14<31:53,  6.42s/it, cF1=0.680, nF1=0.265, rMSE=0.101, pL=0.076]Training (Loss: 0.4812, Grad: 0.31):  12%|█▏        | 41/338 [04:14<34:53,  7.05s/it, cF1=0.680, nF1=0.265, rMSE=0.101, pL=0.076]Training (Loss: 0.3845, Grad: 0.33):  12%|█▏        | 41/338 [04:22<34:53,  7.05s/it, cF1=0.680, nF1=0.265, rMSE=0.101, pL=0.076]Training (Loss: 0.3845, Grad: 0.33):  12%|█▏        | 41/338 [04:22<34:53,  7.05s/it, cF1=0.675, nF1=0.262, rMSE=0.100, pL=0.041]Training (Loss: 0.3845, Grad: 0.33):  12%|█▏        | 42/338 [04:22<35:59,  7.29s/it, cF1=0.675, nF1=0.262, rMSE=0.100, pL=0.041]Training (Loss: 0.4402, Grad: 0.22):  12%|█▏        | 42/338 [04:27<35:59,  7.29s/it, cF1=0.675, nF1=0.262, rMSE=0.100, pL=0.041]Training (Loss: 0.4402, Grad: 0.22):  12%|█▏        | 42/338 [04:27<35:59,  7.29s/it, cF1=0.677, nF1=0.263, rMSE=0.099, pL=0.063]Training (Loss: 0.4402, Grad: 0.22):  13%|█▎        | 43/338 [04:27<31:58,  6.50s/it, cF1=0.677, nF1=0.263, rMSE=0.099, pL=0.063]Training (Loss: 0.4249, Grad: 0.21):  13%|█▎        | 43/338 [04:34<31:58,  6.50s/it, cF1=0.677, nF1=0.263, rMSE=0.099, pL=0.063]Training (Loss: 0.4249, Grad: 0.21):  13%|█▎        | 43/338 [04:34<31:58,  6.50s/it, cF1=0.679, nF1=0.263, rMSE=0.098, pL=0.059]Training (Loss: 0.4249, Grad: 0.21):  13%|█▎        | 44/338 [04:34<32:09,  6.56s/it, cF1=0.679, nF1=0.263, rMSE=0.098, pL=0.059]Training (Loss: 0.4454, Grad: 0.27):  13%|█▎        | 44/338 [04:40<32:09,  6.56s/it, cF1=0.679, nF1=0.263, rMSE=0.098, pL=0.059]Training (Loss: 0.4454, Grad: 0.27):  13%|█▎        | 44/338 [04:40<32:09,  6.56s/it, cF1=0.679, nF1=0.265, rMSE=0.098, pL=0.067]Training (Loss: 0.4454, Grad: 0.27):  13%|█▎        | 45/338 [04:40<32:14,  6.60s/it, cF1=0.679, nF1=0.265, rMSE=0.098, pL=0.067]Training (Loss: 0.4466, Grad: 0.23):  13%|█▎        | 45/338 [04:45<32:14,  6.60s/it, cF1=0.679, nF1=0.265, rMSE=0.098, pL=0.067]Training (Loss: 0.4466, Grad: 0.23):  13%|█▎        | 45/338 [04:45<32:14,  6.60s/it, cF1=0.681, nF1=0.266, rMSE=0.098, pL=0.066]Training (Loss: 0.4466, Grad: 0.23):  14%|█▎        | 46/338 [04:45<29:13,  6.01s/it, cF1=0.681, nF1=0.266, rMSE=0.098, pL=0.066]Training (Loss: 0.4026, Grad: 0.31):  14%|█▎        | 46/338 [04:50<29:13,  6.01s/it, cF1=0.681, nF1=0.266, rMSE=0.098, pL=0.066]Training (Loss: 0.4026, Grad: 0.31):  14%|█▎        | 46/338 [04:50<29:13,  6.01s/it, cF1=0.676, nF1=0.265, rMSE=0.097, pL=0.048]Training (Loss: 0.4026, Grad: 0.31):  14%|█▍        | 47/338 [04:50<28:24,  5.86s/it, cF1=0.676, nF1=0.265, rMSE=0.097, pL=0.048]Training (Loss: 0.4043, Grad: 0.19):  14%|█▍        | 47/338 [04:55<28:24,  5.86s/it, cF1=0.676, nF1=0.265, rMSE=0.097, pL=0.048]Training (Loss: 0.4043, Grad: 0.19):  14%|█▍        | 47/338 [04:55<28:24,  5.86s/it, cF1=0.676, nF1=0.264, rMSE=0.097, pL=0.057]Training (Loss: 0.4043, Grad: 0.19):  14%|█▍        | 48/338 [04:55<25:59,  5.38s/it, cF1=0.676, nF1=0.264, rMSE=0.097, pL=0.057]Training (Loss: 0.3882, Grad: 0.25):  14%|█▍        | 48/338 [05:00<25:59,  5.38s/it, cF1=0.676, nF1=0.264, rMSE=0.097, pL=0.057]Training (Loss: 0.3882, Grad: 0.25):  14%|█▍        | 48/338 [05:00<25:59,  5.38s/it, cF1=0.671, nF1=0.262, rMSE=0.096, pL=0.048]Training (Loss: 0.3882, Grad: 0.25):  14%|█▍        | 49/338 [05:00<25:39,  5.33s/it, cF1=0.671, nF1=0.262, rMSE=0.096, pL=0.048]Training (Loss: 0.4198, Grad: 0.22):  14%|█▍        | 49/338 [05:08<25:39,  5.33s/it, cF1=0.671, nF1=0.262, rMSE=0.096, pL=0.048]Training (Loss: 0.4198, Grad: 0.22):  14%|█▍        | 49/338 [05:08<25:39,  5.33s/it, cF1=0.669, nF1=0.262, rMSE=0.096, pL=0.061]Training (Loss: 0.4198, Grad: 0.22):  15%|█▍        | 50/338 [05:08<29:09,  6.07s/it, cF1=0.669, nF1=0.262, rMSE=0.096, pL=0.061]Training (Loss: 0.4274, Grad: 0.21):  15%|█▍        | 50/338 [05:13<29:09,  6.07s/it, cF1=0.669, nF1=0.262, rMSE=0.096, pL=0.061]Training (Loss: 0.4274, Grad: 0.21):  15%|█▍        | 50/338 [05:13<29:09,  6.07s/it, cF1=0.669, nF1=0.262, rMSE=0.096, pL=0.059]Training (Loss: 0.4274, Grad: 0.21):  15%|█▌        | 51/338 [05:13<28:25,  5.94s/it, cF1=0.669, nF1=0.262, rMSE=0.096, pL=0.059]Training (Loss: 0.4090, Grad: 0.17):  15%|█▌        | 51/338 [05:19<28:25,  5.94s/it, cF1=0.669, nF1=0.262, rMSE=0.096, pL=0.059]Training (Loss: 0.4090, Grad: 0.17):  15%|█▌        | 51/338 [05:19<28:25,  5.94s/it, cF1=0.669, nF1=0.262, rMSE=0.095, pL=0.056]Training (Loss: 0.4090, Grad: 0.17):  15%|█▌        | 52/338 [05:19<27:52,  5.85s/it, cF1=0.669, nF1=0.262, rMSE=0.095, pL=0.056]Training (Loss: 0.4188, Grad: 0.23):  15%|█▌        | 52/338 [05:25<27:52,  5.85s/it, cF1=0.669, nF1=0.262, rMSE=0.095, pL=0.056]Training (Loss: 0.4188, Grad: 0.23):  15%|█▌        | 52/338 [05:25<27:52,  5.85s/it, cF1=0.669, nF1=0.261, rMSE=0.095, pL=0.055]Training (Loss: 0.4188, Grad: 0.23):  16%|█▌        | 53/338 [05:25<27:30,  5.79s/it, cF1=0.669, nF1=0.261, rMSE=0.095, pL=0.055]Training (Loss: 0.4172, Grad: 0.19):  16%|█▌        | 53/338 [05:30<27:30,  5.79s/it, cF1=0.669, nF1=0.261, rMSE=0.095, pL=0.055]Training (Loss: 0.4172, Grad: 0.19):  16%|█▌        | 53/338 [05:30<27:30,  5.79s/it, cF1=0.669, nF1=0.261, rMSE=0.095, pL=0.059]Training (Loss: 0.4172, Grad: 0.19):  16%|█▌        | 54/338 [05:30<26:43,  5.64s/it, cF1=0.669, nF1=0.261, rMSE=0.095, pL=0.059]Training (Loss: 0.4454, Grad: 0.22):  16%|█▌        | 54/338 [05:37<26:43,  5.64s/it, cF1=0.669, nF1=0.261, rMSE=0.095, pL=0.059]Training (Loss: 0.4454, Grad: 0.22):  16%|█▌        | 54/338 [05:37<26:43,  5.64s/it, cF1=0.669, nF1=0.262, rMSE=0.094, pL=0.062]Training (Loss: 0.4454, Grad: 0.22):  16%|█▋        | 55/338 [05:37<28:38,  6.07s/it, cF1=0.669, nF1=0.262, rMSE=0.094, pL=0.062]Training (Loss: 0.4053, Grad: 0.18):  16%|█▋        | 55/338 [05:43<28:38,  6.07s/it, cF1=0.669, nF1=0.262, rMSE=0.094, pL=0.062]Training (Loss: 0.4053, Grad: 0.18):  16%|█▋        | 55/338 [05:43<28:38,  6.07s/it, cF1=0.669, nF1=0.262, rMSE=0.094, pL=0.058]Training (Loss: 0.4053, Grad: 0.18):  17%|█▋        | 56/338 [05:43<28:42,  6.11s/it, cF1=0.669, nF1=0.262, rMSE=0.094, pL=0.058]Training (Loss: 0.3940, Grad: 0.21):  17%|█▋        | 56/338 [05:49<28:42,  6.11s/it, cF1=0.669, nF1=0.262, rMSE=0.094, pL=0.058]Training (Loss: 0.3940, Grad: 0.21):  17%|█▋        | 56/338 [05:49<28:42,  6.11s/it, cF1=0.667, nF1=0.260, rMSE=0.093, pL=0.046]Training (Loss: 0.3940, Grad: 0.21):  17%|█▋        | 57/338 [05:49<28:12,  6.02s/it, cF1=0.667, nF1=0.260, rMSE=0.093, pL=0.046]Training (Loss: 0.4658, Grad: 0.25):  17%|█▋        | 57/338 [05:55<28:12,  6.02s/it, cF1=0.667, nF1=0.260, rMSE=0.093, pL=0.046]Training (Loss: 0.4658, Grad: 0.25):  17%|█▋        | 57/338 [05:55<28:12,  6.02s/it, cF1=0.670, nF1=0.262, rMSE=0.093, pL=0.068]Training (Loss: 0.4658, Grad: 0.25):  17%|█▋        | 58/338 [05:55<27:38,  5.92s/it, cF1=0.670, nF1=0.262, rMSE=0.093, pL=0.068]Training (Loss: 0.4055, Grad: 0.19):  17%|█▋        | 58/338 [06:02<27:38,  5.92s/it, cF1=0.670, nF1=0.262, rMSE=0.093, pL=0.068]Training (Loss: 0.4055, Grad: 0.19):  17%|█▋        | 58/338 [06:02<27:38,  5.92s/it, cF1=0.670, nF1=0.261, rMSE=0.092, pL=0.052]Training (Loss: 0.4055, Grad: 0.19):  17%|█▋        | 59/338 [06:02<29:11,  6.28s/it, cF1=0.670, nF1=0.261, rMSE=0.092, pL=0.052]Training (Loss: 0.4590, Grad: 0.27):  17%|█▋        | 59/338 [06:07<29:11,  6.28s/it, cF1=0.670, nF1=0.261, rMSE=0.092, pL=0.052]Training (Loss: 0.4590, Grad: 0.27):  17%|█▋        | 59/338 [06:07<29:11,  6.28s/it, cF1=0.674, nF1=0.263, rMSE=0.092, pL=0.071]Training (Loss: 0.4590, Grad: 0.27):  18%|█▊        | 60/338 [06:07<27:47,  6.00s/it, cF1=0.674, nF1=0.263, rMSE=0.092, pL=0.071]Training (Loss: 0.4577, Grad: 0.28):  18%|█▊        | 60/338 [06:14<27:47,  6.00s/it, cF1=0.674, nF1=0.263, rMSE=0.092, pL=0.071]Training (Loss: 0.4577, Grad: 0.28):  18%|█▊        | 60/338 [06:14<27:47,  6.00s/it, cF1=0.676, nF1=0.264, rMSE=0.091, pL=0.071]Training (Loss: 0.4577, Grad: 0.28):  18%|█▊        | 61/338 [06:14<28:12,  6.11s/it, cF1=0.676, nF1=0.264, rMSE=0.091, pL=0.071]Training (Loss: 0.4553, Grad: 0.29):  18%|█▊        | 61/338 [06:18<28:12,  6.11s/it, cF1=0.676, nF1=0.264, rMSE=0.091, pL=0.071]Training (Loss: 0.4553, Grad: 0.29):  18%|█▊        | 61/338 [06:18<28:12,  6.11s/it, cF1=0.677, nF1=0.266, rMSE=0.091, pL=0.074]Training (Loss: 0.4553, Grad: 0.29):  18%|█▊        | 62/338 [06:18<26:14,  5.71s/it, cF1=0.677, nF1=0.266, rMSE=0.091, pL=0.074]Training (Loss: 0.4486, Grad: 0.23):  18%|█▊        | 62/338 [06:24<26:14,  5.71s/it, cF1=0.677, nF1=0.266, rMSE=0.091, pL=0.074]Training (Loss: 0.4486, Grad: 0.23):  18%|█▊        | 62/338 [06:25<26:14,  5.71s/it, cF1=0.677, nF1=0.267, rMSE=0.091, pL=0.068]Training (Loss: 0.4486, Grad: 0.23):  19%|█▊        | 63/338 [06:25<27:14,  5.94s/it, cF1=0.677, nF1=0.267, rMSE=0.091, pL=0.068]Training (Loss: 0.3849, Grad: 0.26):  19%|█▊        | 63/338 [06:32<27:14,  5.94s/it, cF1=0.677, nF1=0.267, rMSE=0.091, pL=0.068]Training (Loss: 0.3849, Grad: 0.26):  19%|█▊        | 63/338 [06:32<27:14,  5.94s/it, cF1=0.675, nF1=0.266, rMSE=0.090, pL=0.045]Training (Loss: 0.3849, Grad: 0.26):  19%|█▉        | 64/338 [06:32<28:57,  6.34s/it, cF1=0.675, nF1=0.266, rMSE=0.090, pL=0.045]Training (Loss: 0.4259, Grad: 0.19):  19%|█▉        | 64/338 [06:38<28:57,  6.34s/it, cF1=0.675, nF1=0.266, rMSE=0.090, pL=0.045]Training (Loss: 0.4259, Grad: 0.19):  19%|█▉        | 64/338 [06:38<28:57,  6.34s/it, cF1=0.675, nF1=0.266, rMSE=0.090, pL=0.065]Training (Loss: 0.4259, Grad: 0.19):  19%|█▉        | 65/338 [06:38<28:17,  6.22s/it, cF1=0.675, nF1=0.266, rMSE=0.090, pL=0.065]Training (Loss: 0.4149, Grad: 0.23):  19%|█▉        | 65/338 [06:44<28:17,  6.22s/it, cF1=0.675, nF1=0.266, rMSE=0.090, pL=0.065]Training (Loss: 0.4149, Grad: 0.23):  19%|█▉        | 65/338 [06:44<28:17,  6.22s/it, cF1=0.675, nF1=0.266, rMSE=0.090, pL=0.056]Training (Loss: 0.4149, Grad: 0.23):  20%|█▉        | 66/338 [06:44<27:32,  6.07s/it, cF1=0.675, nF1=0.266, rMSE=0.090, pL=0.056]Training (Loss: 0.4720, Grad: 0.22):  20%|█▉        | 66/338 [06:50<27:32,  6.07s/it, cF1=0.675, nF1=0.266, rMSE=0.090, pL=0.056]Training (Loss: 0.4720, Grad: 0.22):  20%|█▉        | 66/338 [06:50<27:32,  6.07s/it, cF1=0.677, nF1=0.269, rMSE=0.090, pL=0.077]Training (Loss: 0.4720, Grad: 0.22):  20%|█▉        | 67/338 [06:50<27:43,  6.14s/it, cF1=0.677, nF1=0.269, rMSE=0.090, pL=0.077]Training (Loss: 0.4720, Grad: 0.27):  20%|█▉        | 67/338 [06:56<27:43,  6.14s/it, cF1=0.677, nF1=0.269, rMSE=0.090, pL=0.077]Training (Loss: 0.4720, Grad: 0.27):  20%|█▉        | 67/338 [06:56<27:43,  6.14s/it, cF1=0.678, nF1=0.270, rMSE=0.090, pL=0.071]Training (Loss: 0.4720, Grad: 0.27):  20%|██        | 68/338 [06:56<28:03,  6.24s/it, cF1=0.678, nF1=0.270, rMSE=0.090, pL=0.071]Training (Loss: 0.4630, Grad: 0.23):  20%|██        | 68/338 [07:03<28:03,  6.24s/it, cF1=0.678, nF1=0.270, rMSE=0.090, pL=0.071]Training (Loss: 0.4630, Grad: 0.23):  20%|██        | 68/338 [07:03<28:03,  6.24s/it, cF1=0.681, nF1=0.272, rMSE=0.089, pL=0.072]Training (Loss: 0.4630, Grad: 0.23):  20%|██        | 69/338 [07:03<28:50,  6.43s/it, cF1=0.681, nF1=0.272, rMSE=0.089, pL=0.072]Training (Loss: 0.4511, Grad: 0.20):  20%|██        | 69/338 [07:11<28:50,  6.43s/it, cF1=0.681, nF1=0.272, rMSE=0.089, pL=0.072]Training (Loss: 0.4511, Grad: 0.20):  20%|██        | 69/338 [07:11<28:50,  6.43s/it, cF1=0.682, nF1=0.273, rMSE=0.089, pL=0.068]Training (Loss: 0.4511, Grad: 0.20):  21%|██        | 70/338 [07:11<29:57,  6.71s/it, cF1=0.682, nF1=0.273, rMSE=0.089, pL=0.068]Training (Loss: 0.4013, Grad: 0.20):  21%|██        | 70/338 [07:16<29:57,  6.71s/it, cF1=0.682, nF1=0.273, rMSE=0.089, pL=0.068]Training (Loss: 0.4013, Grad: 0.20):  21%|██        | 70/338 [07:16<29:57,  6.71s/it, cF1=0.682, nF1=0.272, rMSE=0.088, pL=0.055]Training (Loss: 0.4013, Grad: 0.20):  21%|██        | 71/338 [07:16<28:13,  6.34s/it, cF1=0.682, nF1=0.272, rMSE=0.088, pL=0.055]Training (Loss: 0.4474, Grad: 0.22):  21%|██        | 71/338 [07:22<28:13,  6.34s/it, cF1=0.682, nF1=0.272, rMSE=0.088, pL=0.055]Training (Loss: 0.4474, Grad: 0.22):  21%|██        | 71/338 [07:22<28:13,  6.34s/it, cF1=0.683, nF1=0.273, rMSE=0.088, pL=0.071]Training (Loss: 0.4474, Grad: 0.22):  21%|██▏       | 72/338 [07:22<27:05,  6.11s/it, cF1=0.683, nF1=0.273, rMSE=0.088, pL=0.071]Training (Loss: 0.3956, Grad: 0.25):  21%|██▏       | 72/338 [07:24<27:05,  6.11s/it, cF1=0.683, nF1=0.273, rMSE=0.088, pL=0.071]Training (Loss: 0.3956, Grad: 0.25):  21%|██▏       | 72/338 [07:24<27:05,  6.11s/it, cF1=0.683, nF1=0.273, rMSE=0.088, pL=0.053]Training (Loss: 0.3956, Grad: 0.25):  22%|██▏       | 73/338 [07:24<21:57,  4.97s/it, cF1=0.683, nF1=0.273, rMSE=0.088, pL=0.053]Training (Loss: 0.4155, Grad: 0.15):  22%|██▏       | 73/338 [07:30<21:57,  4.97s/it, cF1=0.683, nF1=0.273, rMSE=0.088, pL=0.053]Training (Loss: 0.4155, Grad: 0.15):  22%|██▏       | 73/338 [07:30<21:57,  4.97s/it, cF1=0.683, nF1=0.273, rMSE=0.087, pL=0.065]Training (Loss: 0.4155, Grad: 0.15):  22%|██▏       | 74/338 [07:30<23:23,  5.32s/it, cF1=0.683, nF1=0.273, rMSE=0.087, pL=0.065]Training (Loss: 0.3878, Grad: 0.34):  22%|██▏       | 74/338 [07:35<23:23,  5.32s/it, cF1=0.683, nF1=0.273, rMSE=0.087, pL=0.065]Training (Loss: 0.3878, Grad: 0.34):  22%|██▏       | 74/338 [07:35<23:23,  5.32s/it, cF1=0.681, nF1=0.272, rMSE=0.087, pL=0.054]Training (Loss: 0.3878, Grad: 0.34):  22%|██▏       | 75/338 [07:35<23:07,  5.28s/it, cF1=0.681, nF1=0.272, rMSE=0.087, pL=0.054]Training (Loss: 0.4268, Grad: 0.22):  22%|██▏       | 75/338 [07:41<23:07,  5.28s/it, cF1=0.681, nF1=0.272, rMSE=0.087, pL=0.054]Training (Loss: 0.4268, Grad: 0.22):  22%|██▏       | 75/338 [07:41<23:07,  5.28s/it, cF1=0.681, nF1=0.272, rMSE=0.087, pL=0.060]Training (Loss: 0.4268, Grad: 0.22):  22%|██▏       | 76/338 [07:41<24:03,  5.51s/it, cF1=0.681, nF1=0.272, rMSE=0.087, pL=0.060]Training (Loss: 0.4155, Grad: 0.17):  22%|██▏       | 76/338 [07:49<24:03,  5.51s/it, cF1=0.681, nF1=0.272, rMSE=0.087, pL=0.060]Training (Loss: 0.4155, Grad: 0.17):  22%|██▏       | 76/338 [07:49<24:03,  5.51s/it, cF1=0.681, nF1=0.273, rMSE=0.086, pL=0.069]Training (Loss: 0.4155, Grad: 0.17):  23%|██▎       | 77/338 [07:49<26:29,  6.09s/it, cF1=0.681, nF1=0.273, rMSE=0.086, pL=0.069]Training (Loss: 0.4028, Grad: 0.26):  23%|██▎       | 77/338 [07:56<26:29,  6.09s/it, cF1=0.681, nF1=0.273, rMSE=0.086, pL=0.069]Training (Loss: 0.4028, Grad: 0.26):  23%|██▎       | 77/338 [07:56<26:29,  6.09s/it, cF1=0.681, nF1=0.273, rMSE=0.086, pL=0.055]Training (Loss: 0.4028, Grad: 0.26):  23%|██▎       | 78/338 [07:56<27:56,  6.45s/it, cF1=0.681, nF1=0.273, rMSE=0.086, pL=0.055]Training (Loss: 0.3784, Grad: 0.39):  23%|██▎       | 78/338 [08:03<27:56,  6.45s/it, cF1=0.681, nF1=0.273, rMSE=0.086, pL=0.055]Training (Loss: 0.3784, Grad: 0.39):  23%|██▎       | 78/338 [08:03<27:56,  6.45s/it, cF1=0.678, nF1=0.271, rMSE=0.086, pL=0.050]Training (Loss: 0.3784, Grad: 0.39):  23%|██▎       | 79/338 [08:03<28:12,  6.53s/it, cF1=0.678, nF1=0.271, rMSE=0.086, pL=0.050]Training (Loss: 0.3696, Grad: 0.30):  23%|██▎       | 79/338 [08:08<28:12,  6.53s/it, cF1=0.678, nF1=0.271, rMSE=0.086, pL=0.050]Training (Loss: 0.3696, Grad: 0.30):  23%|██▎       | 79/338 [08:08<28:12,  6.53s/it, cF1=0.675, nF1=0.270, rMSE=0.085, pL=0.046]Training (Loss: 0.3696, Grad: 0.30):  24%|██▎       | 80/338 [08:08<25:57,  6.04s/it, cF1=0.675, nF1=0.270, rMSE=0.085, pL=0.046]Training (Loss: 0.3421, Grad: 0.35):  24%|██▎       | 80/338 [08:15<25:57,  6.04s/it, cF1=0.675, nF1=0.270, rMSE=0.085, pL=0.046]Training (Loss: 0.3421, Grad: 0.35):  24%|██▎       | 80/338 [08:15<25:57,  6.04s/it, cF1=0.671, nF1=0.267, rMSE=0.085, pL=0.037]Training (Loss: 0.3421, Grad: 0.35):  24%|██▍       | 81/338 [08:15<27:38,  6.45s/it, cF1=0.671, nF1=0.267, rMSE=0.085, pL=0.037]Training (Loss: 0.3945, Grad: 0.13):  24%|██▍       | 81/338 [08:23<27:38,  6.45s/it, cF1=0.671, nF1=0.267, rMSE=0.085, pL=0.037]Training (Loss: 0.3945, Grad: 0.13):  24%|██▍       | 81/338 [08:23<27:38,  6.45s/it, cF1=0.669, nF1=0.267, rMSE=0.085, pL=0.057]Training (Loss: 0.3945, Grad: 0.13):  24%|██▍       | 82/338 [08:23<29:03,  6.81s/it, cF1=0.669, nF1=0.267, rMSE=0.085, pL=0.057]Training (Loss: 0.4315, Grad: 0.20):  24%|██▍       | 82/338 [08:28<29:03,  6.81s/it, cF1=0.669, nF1=0.267, rMSE=0.085, pL=0.057]Training (Loss: 0.4315, Grad: 0.20):  24%|██▍       | 82/338 [08:28<29:03,  6.81s/it, cF1=0.671, nF1=0.268, rMSE=0.084, pL=0.067]Training (Loss: 0.4315, Grad: 0.20):  25%|██▍       | 83/338 [08:28<26:30,  6.24s/it, cF1=0.671, nF1=0.268, rMSE=0.084, pL=0.067]Training (Loss: 0.3995, Grad: 0.16):  25%|██▍       | 83/338 [08:33<26:30,  6.24s/it, cF1=0.671, nF1=0.268, rMSE=0.084, pL=0.067]Training (Loss: 0.3995, Grad: 0.16):  25%|██▍       | 83/338 [08:33<26:30,  6.24s/it, cF1=0.671, nF1=0.267, rMSE=0.084, pL=0.052]Training (Loss: 0.3995, Grad: 0.16):  25%|██▍       | 84/338 [08:33<25:26,  6.01s/it, cF1=0.671, nF1=0.267, rMSE=0.084, pL=0.052]Training (Loss: 0.4046, Grad: 0.16):  25%|██▍       | 84/338 [08:40<25:26,  6.01s/it, cF1=0.671, nF1=0.267, rMSE=0.084, pL=0.052]Training (Loss: 0.4046, Grad: 0.16):  25%|██▍       | 84/338 [08:40<25:26,  6.01s/it, cF1=0.669, nF1=0.268, rMSE=0.084, pL=0.062]Training (Loss: 0.4046, Grad: 0.16):  25%|██▌       | 85/338 [08:40<26:35,  6.31s/it, cF1=0.669, nF1=0.268, rMSE=0.084, pL=0.062]